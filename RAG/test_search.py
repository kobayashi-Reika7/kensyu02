"""
æœ€å°å‹•ä½œç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆAPIã‚­ãƒ¼ä¸è¦ï¼‰
=======================================
Embedding + ChromaDB ã®æ¤œç´¢éƒ¨åˆ†ã®ã¿ã‚’ç¢ºèªã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
LLMï¼ˆOpenAIï¼‰ã®å›ç­”ç”Ÿæˆã¯è¡Œã‚ãªã„ã®ã§ã€APIã‚­ãƒ¼ãªã—ã§å®Ÿè¡Œå¯èƒ½ã€‚

å®Ÿè¡Œæ–¹æ³•:
  python test_search.py

ç¢ºèªã§ãã‚‹ã“ã¨:
  - Embeddingãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»å‹•ä½œã™ã‚‹ã‹
  - ãƒ†ã‚­ã‚¹ãƒˆãŒãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚Œã‚‹ã‹
  - ChromaDBã«ä¿å­˜ã•ã‚Œã‚‹ã‹
  - è³ªå•ã«å¯¾ã—ã¦æ­£ã—ã„ãƒãƒ£ãƒ³ã‚¯ãŒæ¤œç´¢ã•ã‚Œã‚‹ã‹
"""

import os
from src.text_splitter_utils import create_token_text_splitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document


def main():
    print("=" * 60)
    print("â™¨ï¸ OnsenRAG æœ€å°å‹•ä½œç¢ºèªï¼ˆAPIã‚­ãƒ¼ä¸è¦ï¼‰")
    print("=" * 60)

    # ã‚¹ãƒ†ãƒƒãƒ—1: æ¸©æ³‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    data_path = os.path.join(os.path.dirname(__file__), "data", "onsen_knowledge.txt")

    if not os.path.exists(data_path):
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {data_path}")
        return

    with open(data_path, "r", encoding="utf-8") as f:
        text = f.read()

    print(f"\nğŸ“„ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿è¾¼ã¿å®Œäº† ({len(text)}æ–‡å­—)")

    # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–
    text_splitter = create_token_text_splitter(
        chunk_size=450,
        chunk_overlap=75
        separators=["â–  ", "\n\n", "\n", "ã€‚", "ã€", " ", ""]
    )
    document = Document(page_content=text)
    splits = text_splitter.split_documents([document])

    print(f"âœ‚ï¸ ã‚¹ãƒ†ãƒƒãƒ—2: {len(splits)}ä»¶ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")
    for i, split in enumerate(splits):
        preview = split.page_content[:50].replace("\n", " ")
        print(f"  [{i+1}] {preview}...")

    # ã‚¹ãƒ†ãƒƒãƒ—3: Embeddingãƒ¢ãƒ‡ãƒ«ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ– â†’ ChromaDBã«ä¿å­˜
    print("\nâ³ ã‚¹ãƒ†ãƒƒãƒ—3: Embeddingãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    print("  ï¼ˆåˆå›ã¯æ¨¡å‹ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰")

    embeddings = HuggingFaceEmbeddings(
        model_name="intfloat/multilingual-e5-base"
    )

    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings
    )
    print("âœ… ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ™ã‚¯ãƒˆãƒ«DBã«ä¿å­˜å®Œäº†")

    # ã‚¹ãƒ†ãƒƒãƒ—4: è³ªå•ã«å¯¾ã—ã¦æ¤œç´¢ãƒ†ã‚¹ãƒˆ
    print("\n" + "=" * 60)
    print("ğŸ” ã‚¹ãƒ†ãƒƒãƒ—4: æ¤œç´¢ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)

    test_questions = [
        "æ¸©æ³‰ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "ç¾è‚Œã®æ¹¯ã¨å‘¼ã°ã‚Œã‚‹æ³‰è³ªã¯ï¼Ÿ",
        "å†¬ã«ãŠã™ã™ã‚ã®æ¸©æ³‰åœ°ã¯ï¼Ÿ",
        "æ±äº¬ã‹ã‚‰è¡Œãã‚„ã™ã„æ¸©æ³‰åœ°ã¯ï¼Ÿ",
        "åˆºæ¿€ãŒå°‘ãªã„æ¸©æ³‰ã¯ã©ã‚Œã§ã™ã‹ï¼Ÿ",
        "æ¹¯ã‚ãã‚Šã‚’æ¥½ã—ã‚ã‚‹æ¸©æ³‰åœ°ã¯ï¼Ÿ",
    ]

    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

    for question in test_questions:
        print(f"\nâ“ è³ªå•: {question}")
        docs = retriever.invoke(question)
        for i, doc in enumerate(docs):
            content = doc.page_content.replace("\n", " ")
            preview = content[:80] + "..." if len(content) > 80 else content
            print(f"  [{i+1}] {preview}")

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 60)
    print("âœ… å‹•ä½œç¢ºèªå®Œäº†ï¼")
    print("=" * 60)
    print()
    print("ç¢ºèªã§ããŸã“ã¨:")
    print("  âœ… ãƒ†ã‚­ã‚¹ãƒˆã®èª­ã¿è¾¼ã¿ãƒ»ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²")
    print("  âœ… Embeddingãƒ¢ãƒ‡ãƒ«ï¼ˆmultilingual-e5-baseï¼‰ã®å‹•ä½œ")
    print("  âœ… ChromaDB ã¸ã®ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜")
    print("  âœ… è³ªå•ã«å¯¾ã™ã‚‹é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æ¤œç´¢")
    print()
    print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. .envãƒ•ã‚¡ã‚¤ãƒ«ã«OpenAI APIã‚­ãƒ¼ã‚’è¨­å®š")
    print("     OPENAI_API_KEY=sk-your-actual-key")
    print("  2. python main.py ã§å®Œå…¨ãªRAGãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ")
    print("  3. uvicorn api.main:app --reload --port 8000 ã§ãƒãƒ£ãƒƒãƒˆAPIèµ·å‹•")


if __name__ == "__main__":
    main()
